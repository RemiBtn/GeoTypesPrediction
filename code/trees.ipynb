{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import date\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csvs\n",
    "\n",
    "train_df = gpd.read_file('train.geojson', index_col=0)\n",
    "test_df = gpd.read_file('test.geojson', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310006, 13)\n",
      "(121704, 13)\n"
     ]
    }
   ],
   "source": [
    "# Filtering columns on sets\n",
    "le = LabelEncoder()\n",
    "\n",
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4, 'Mega Projects': 5}\n",
    "y_train = pd.DataFrame(train_df['change_type'].apply(lambda x: change_type_map[x]))\n",
    "\n",
    "def get_x(df):\n",
    "    x = pd.DataFrame()\n",
    "\n",
    "    # status dates\n",
    "    for i in range(1,6):\n",
    "        x['change_status_date{}'.format(i)] = le.fit_transform(df['change_status_date{}'.format(i)])\n",
    "\n",
    "    # dates\n",
    "    # for i in range(1,6):\n",
    "    #     x['year{}'.format(i)] = le.fit_transform(df['date{}'.format(i)].transform(lambda x: x[-4:-1]+x[-1]))\n",
    "    # for i in range(1,6):\n",
    "    #     x['month{}'.format(i)] = le.fit_transform(df['date{}'.format(i)].transform(lambda x: x[-7:-5]))\n",
    "    # x['delta_year'] = train_df['date5'].transform(lambda x: x[-4:-1]+x[-1]).astype(int).subtract(train_df['date1'].transform(lambda x: x[-4:-1]+x[-1]).astype(int))\n",
    "\n",
    "    # times intervals\n",
    "    # def date_type(t1,t2):\n",
    "    #     t = np.zeros(len(t1))\n",
    "    #     for i in range(len(t1)):\n",
    "    #         t1[i]=\"-\".join([t1[i][6:10],t1[i][3:5],t1[i][:2]])\n",
    "    #         t2[i]=\"-\".join([t2[i][6:10],t2[i][3:5],t2[i][:2]])\n",
    "    #         t[i]=(date.fromisoformat(t2[i])-date.fromisoformat(t1[i])).days\n",
    "    #     return t\n",
    "    # for i in range(1,5):\n",
    "    #     t1 = np.array(df['date{}'.format(i)])\n",
    "    #     t2 = np.array(df['date{}'.format(i+1)])\n",
    "    #     x['t_int{}'.format(i)] = date_type(t1,t2)\n",
    "\n",
    "    # total projet time\n",
    "    # total_project_time=[]\n",
    "    # for k in range(df.shape[0]):\n",
    "    #     t_start=1\n",
    "    #     i=2\n",
    "    #     while i<6 and x['change_status_date{}'.format(i)][k] == x['change_status_date{}'.format(i-1)][k] and i<6:\n",
    "    #         t_start=i\n",
    "    #         i+=1\n",
    "    #     t_end = 5\n",
    "\n",
    "    #     i = 5\n",
    "    #     while i>1 and x['change_status_date{}'.format(i)][k] == x['change_status_date{}'.format(i-1)][k] :\n",
    "    #         t_start = i-1\n",
    "    #         i -= 1\n",
    "    #     project_time=0\n",
    "    #     for i in range(t_start,t_end):\n",
    "    #         project_time += x['t_int{}'.format(i)][k]\n",
    "    #     total_project_time.append(project_time)\n",
    "    # x['total_project_time'] = np.array(total_project_time)\n",
    "\n",
    "    # urban types\n",
    "    x['urban_types'] = le.fit_transform(df['urban_types'])\n",
    "\n",
    "    # geography types\n",
    "    # geography_types_map = {'River': 0,'Sparse Forest': 1,'Grass Land': 2,'Farms': 3,'Lakes': 4,'Barren Land': 5,'Coastal': 6,'Dense Forest': 7,'None': 8,'Hills': 9,'Desert': 10,'Snow': 11}\n",
    "    # dic_geo = {}\n",
    "    # for geography_type in geography_types_map:\n",
    "    #     dic_geo[geography_type] = np.zeros(df.shape[0])\n",
    "\n",
    "    # geo = np.array(df['geography_types'])\n",
    "    # for k in range(len(geo)):\n",
    "    #     geo[k] = geo[k].split(',')\n",
    "    #     for geo_type in geo[k]:\n",
    "    #         dic_geo[geo_type][k] = 1\n",
    "\n",
    "    # for geography_type in geography_types_map:\n",
    "    #     x[geography_type] = dic_geo[geography_type]\n",
    "\n",
    "    # geometry features\n",
    "    # area\n",
    "    x['area'] = df[['geometry']].area\n",
    "    # perimeter\n",
    "    x['perimeter'] = df[['geometry']].length\n",
    "    # centroid of the polygon\n",
    "    x['x_centroid'] = df[['geometry']].centroid.x\n",
    "    x['y_centroid'] = df[['geometry']].centroid.y\n",
    "    # # length on the x and y axis\n",
    "    x['delta_x'] = df[['geometry']].bounds.maxx.subtract(df[['geometry']].bounds.minx)\n",
    "    x['delta_y'] = df[['geometry']].bounds.maxy.subtract(df[['geometry']].bounds.miny)\n",
    "    # angles\n",
    "    x['angle_diago'] = (x['delta_x'].div((x['delta_x'].apply(lambda x: x**2) + x['delta_y'].apply(lambda x: x**2)).apply(np.sqrt))).apply(np.arccos)\n",
    "    # def coord_lister(geom):\n",
    "    #     coords = list(geom.exterior.coords)\n",
    "    #     return (coords)\n",
    "    # coordinates_list = pd.DataFrame(train_df.geometry.apply(coord_lister))\n",
    "    # x1 = coordinates_list.apply(lambda x: x[0][0][0], axis=1)\n",
    "    # y1 = coordinates_list.apply(lambda x: x[0][0][1], axis=1)\n",
    "    # x2 = coordinates_list.apply(lambda x: x[0][3][0], axis=1)\n",
    "    # y2 = coordinates_list.apply(lambda x: x[0][3][1], axis=1)\n",
    "    # x['angle'] = (x1*x2 + y1*y2).div((x1**2 + y1**2).apply(np.sqrt) * (x2**2 + y2**2).apply(np.sqrt)).apply(lambda x: x*np.pi/180).apply(np.arccos)\n",
    "\n",
    "    return x\n",
    "\n",
    "x_train = get_x(train_df)\n",
    "x_test = get_x(test_df)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling to tackle unbalanced data\n",
    "over_sampling = False\n",
    "under_sampling = False\n",
    "\n",
    "if over_sampling:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "    samplers = [SMOTE(random_state=0), SMOTEENN(random_state=0), SMOTETomek(random_state=0)]\n",
    "    sampler = samplers[2]\n",
    "    x_res, y_res = sampler.fit_resample(x_train, y_train)\n",
    "    print(x_res.shape)\n",
    "\n",
    "if under_sampling:\n",
    "    from imblearn.pipeline import make_pipeline\n",
    "    from imblearn.metrics import classification_report_imbalanced\n",
    "    from imblearn.under_sampling import NearMiss\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = make_pipeline(NearMiss(version=2), StandardScaler(), LogisticRegression(random_state=42))\n",
    "    pipeline.fit(x_train, y_train)\n",
    "\n",
    "    # Classify and report the results\n",
    "    # print(classification_report_imbalanced(y_test, pipeline.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310006, 92)\n"
     ]
    }
   ],
   "source": [
    "polynomial_features = True\n",
    "if polynomial_features:\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "    poly = PolynomialFeatures(2, interaction_only=True)\n",
    "    x_train = poly.fit_transform(x_train)\n",
    "    x_test = poly.fit_transform(x_test)\n",
    "    print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the important features with PCA\n",
    "\n",
    "gridsearch = False\n",
    "indices = range(1, x_train.shape[1], 10)\n",
    "if gridsearch:\n",
    "    s=[]\n",
    "    for n in indices:\n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(x_train)\n",
    "        s.append(pca.score(x_train))\n",
    "        \n",
    "    plt.plot(list(indices),s)\n",
    "\n",
    "do_PCA = False\n",
    "if do_PCA:\n",
    "    pca = PCA(n_components=20)\n",
    "    x_train_reduced = pca.fit_transform(x_train)\n",
    "    x_test_reduced = pca.fit_transform(x_test)\n",
    "    print(x_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217004, 92)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting the data\n",
    "random_state = 42\n",
    "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train, y_train, test_size=0.3, random_state=random_state)\n",
    "x_train_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the classifier\n",
    "\n",
    "tree = False\n",
    "random_forest = True\n",
    "adaboost = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple decision tree\n",
    "\n",
    "if tree:\n",
    "    clf = DecisionTreeClassifier(max_depth=8)\n",
    "    clf.fit(x_train_split, y_train_split)\n",
    "\n",
    "    # predict & assess\n",
    "    y_pred_train = clf.predict(x_train_split)\n",
    "    y_pred_test = clf.predict(x_test_split)\n",
    "\n",
    "    print(accuracy_score(y_train_split, y_pred_train))\n",
    "    print(accuracy_score(y_test_split, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "\n",
    "if random_forest:\n",
    "    rf_clf = RandomForestClassifier(max_depth=15, random_state=0)\n",
    "    rf_clf.fit(x_train, y_train)\n",
    "\n",
    "    # predict & assess\n",
    "    y_pred_train_rf = rf_clf.predict(x_train)\n",
    "    # y_pred_test_rf = rf_clf.predict(x_test_split)\n",
    "\n",
    "    print(accuracy_score(y_train, y_pred_train_rf))\n",
    "    # print(accuracy_score(y_test_split, y_pred_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remib\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6508486932510984\n"
     ]
    }
   ],
   "source": [
    "# Adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "if adaboost:\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=50, random_state=0)\n",
    "    ab_clf.fit(x_train, y_train)\n",
    "\n",
    "    # predict & assess\n",
    "    y_pred_train_ab = ab_clf.predict(x_train)\n",
    "    # y_pred_test_ab = ab_clf.predict(x_test_split)\n",
    "\n",
    "    print(accuracy_score(y_train, y_pred_train_ab))\n",
    "    # print(accuracy_score(y_test_split, y_pred_test_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to submission file\n",
    "\n",
    "def save_results(clf, name_file, x_kaggle=x_test):\n",
    "    y_pred_kaggle = pd.DataFrame(clf.predict(x_kaggle), columns=['change_type'])\n",
    "    y_pred_kaggle.to_csv(name_file+\"_sample_submission.csv\", index=True, index_label='Id')\n",
    "\n",
    "save_results(ab_clf, \"adaboost\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4dd97f6e63f3ceeed1d89e97c28c3f49dd4b825c09c4f028583faf315403134"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
